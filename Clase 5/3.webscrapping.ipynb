{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scrapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flujo de trabajo:\n",
    "\n",
    "- Pedir la página web al servidor. \n",
    "- Parsearla; identificar y extraer los elementos que nos interesan. \n",
    "- Guardar la información extraída. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herramientas a utilizar:\n",
    "Para hacer webscrapping en python, necesitamos las siguientes herramientas: \n",
    "\n",
    "1. Para pedir información:\n",
    "    - requests\n",
    "2. Para parsear la información\n",
    "    - BeautifulSoup\n",
    "3. Para lo demás:  \n",
    "    - pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requests\n",
    "\n",
    "Vital para interactuar con objetos html (como en APIS).\n",
    "\n",
    "### Headers:\n",
    "los headers son clave, pues simularán parámetros para hacer nuestra búsqueda (e.g. Tipo de navegador, versión, dispositivo, caching, redirecciones, sesiones, etc). Puedes encontrar más [aquí]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'http://httpbin.org/headers'\n",
    "resp = requests.get(URL)\n",
    "\n",
    "print('Respuesta sin headers:')\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Respuesta con headers:')\n",
    "posibles_headers =[\n",
    "{\n",
    " 'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36'\n",
    "},\n",
    "{\n",
    "'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/537.36'\n",
    "},\n",
    "{\n",
    "  'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:107.0) Gecko/20100101 Firefox/107.0'  \n",
    "}\n",
    "]\n",
    "    \n",
    "resp_con_headers = requests.get(URL, headers = posibles_headers[2])\n",
    "print(resp_con_headers.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pedimos info a wikipedia\n",
    "url = 'https://es.wikipedia.org/wiki/Wikipedia:Portada'\n",
    "\n",
    "# Guardamos el objeto que nos devuelve\n",
    "respuesta = requests.get(url)\n",
    "\n",
    "print(f'Tipo de Objeto: {type(respuesta)} \\n')\n",
    "print(f'Código de estado: {respuesta.status_code} \\n')\n",
    "print(f'Data: {respuesta.text} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "###  Detour: Utilizando requests para otros fines:\n",
    "Requests nos puede ayudar a descargar todo tipo de contenidos, como imágenes, videos, documentos. En el ejemplo descargamos una presentación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppt = requests.get(\"https://github.com/ccsuehara/varios/raw/refs/heads/main/presentacion.pptx\")\n",
    "\n",
    "with open('ejemplo.pptx', 'wb') as file:\n",
    "    for chunk in ppt.iter_content(chunk_size=1024): ## bajando información por chunks, en vez de todo junto \n",
    "        file.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigamos: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navegando por el DOM como si fuera una Sopa de etiquetas.\n",
    " ![\"beautiful soup\"](webscrapdom.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etiquetas más importantes a buscar\n",
    "\n",
    "- `div` - secciones grandes.  \n",
    "- `table` - tablas, divididas en   `tr` (filas) and `td` (datum).  \n",
    "- `form` - etiqueta input para ser enviada.  \n",
    "- `ul`/`ol` -listas (no ordenadas y ordenadas), contiene `li` (items de listas).  \n",
    "\n",
    "Atributos importantes: \n",
    "- `id` y `class`\n",
    "Los tags se llaman diferente aquí; los web developers utilizan estos tags para diseño e interacción. \n",
    "- `ids` son únicos; `classes` son grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.scrapethissite.com/pages/simple/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Obtenemos el HTML\n",
    "respuesta  = requests.get(url)\n",
    "## 2. Volvemos HTML a string. \n",
    "html_texto = respuesta.text\n",
    "## 3. Parseando el HTML\n",
    "soup = BeautifulSoup(html_texto, \"html.parser\") ## puede ser \"lxml\" o \"html5lib\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método `find`\n",
    "Sólo el primer elemento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"h3\").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### El método `find_all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_h3 = soup.find_all(\"h3\") ## nos  da todoslos h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando el get_text\n",
    "for seccion in all_h3:\n",
    "  print(seccion.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obteniendo lo mismo pero con nombre de las clases\n",
    "# Clase\n",
    "h3_class = soup.find_all('h3', class_  = \"country-name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizando el get_text\n",
    "for seccion in h3_class:\n",
    "  print(seccion.get_text(strip=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## almacenando la información\n",
    "paises = [pais.get_text(strip=True) for pais in h3_class]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obteniendo poblacion. \n",
    "h3_population = soup.find_all(\"span\", class_ = \"country-population\")\n",
    "poblacion = [pop.get_text(strip=True) for pop in h3_population]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios:\n",
    "- Realiza el mismo ejercicio para hallar la capital y el area en km2\n",
    "- Guarda el resultado en un dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrap en 3 líneas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscaremos titulares en portada de el Comercio. \n",
    "r = requests.get('https://elcomercio.pe/', headers = posibles_headers[1])\n",
    "soup = BeautifulSoup(r.text)\n",
    "tags = soup.find_all('h2', class_='fs-wi__title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "- Identifica 2 webs de noticias que te interesen. \n",
    "- Encuentra la etiqueta que refieren a los titulares de portada de cada una. \n",
    "- Extrae los titulares con técnicas de webscrapping. \n",
    "- Guárdalo en un dataframe. Tus columnas pueden ser:\n",
    "    - nombre del periódico\n",
    "    - fecha y hora de la obtencion\n",
    "    - titulos  \n",
    "\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "strmlt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

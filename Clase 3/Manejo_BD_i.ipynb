{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Manejo de bases de datos para las ciencias sociales I\n",
    "\n",
    "## Introducción \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introducción \n",
    "\n",
    "_Previo: Tipos de datos._ \n",
    "\n",
    "Al 2025, Python tiene una serie de librerías que manejan bases de datos tabulares <sup>1</sup>. \n",
    "\n",
    "[Pandas](https://pandas.pydata.org/docs/getting_started/index.html) es la librería más popular de Python para realizar análisis de datos. Esta se especializa en trabajar con ciertos tipos de datos. Los más importantes son:\n",
    "\n",
    "- Data tabular, como los dataframes (o cualquier base de datos estructurada que tengamos).\n",
    "- Series\n",
    "- Series de tiempo\n",
    "\n",
    "Los objetos de Pandas más importantes son 2: \n",
    "- Series\n",
    "- Dataframes (nuestro enfoque principal). \n",
    "\n",
    "<sup>1</sup> Recientemente, la discusión ha virado a mejores métodos para tratar big data. Paquetes como dask, y tipos de archivos binarios como parquet, son un punto inicial. Lo importante es lo siguiente: Son extensiones de Pandas! :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Esta librería es un _must_ en el aprendizaje del análisis de datos, machine learning, etc por una serie de motivos:\n",
    "- Se integra perfectamente con librerías como scikit-learn (machine learning), matplotlib, seaborn y altair (visualización de datos), statsmodels (modelos estadísticos listos para usar), análisis de texto, análisis espacial (geopandas, etc).\n",
    "\n",
    "- La gran parte de la comunidad que hace data analysis, y que se apoya mediante foros y demás, usa Pandas. \n",
    "- Pandas es muy amplio y tiene muchas funcionalidades. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Importar pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Hacer disponible nuestro dataframe\n",
    "\n",
    "- Casi siempre trabajaremos con dataframes. \n",
    "\n",
    "Pero qué es un dataframe? \n",
    "\n",
    "\n",
    "- Es el **objeto** principal de pandas, el cual representa una estructura de datos de dos dimensiones: filas y columnas. \n",
    "- Cada fila es una observación o registro;  cada columna, una variable, atributo, _feature_, etc.\n",
    "- Hay un sólo valor por celda.   \n",
    "- El dataframe sirve para representar datos tabulares. \n",
    "\n",
    "\n",
    "Adicionalmente, todo dataframe tiene un **índice**, que funge como identificador único de cada fila. \n",
    "Las columnas no necesitan ello, porque por regla, los nombres ya son su identificador único. Por ello, no se acepta nombres repetidos de columna. \n",
    "\n",
    "2 maneras de usar dataframes: crearlo por nosotros mismos ó leer datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"../img/db_example.jpeg\" width=\"800\">\n",
    "\n",
    "\n",
    "\n",
    "<br>\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "####  Una forma simple de crear dataframes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nombres_columnas = ['rural', 'pobreza', 'num_ccpp_urbano', 'desnutricion_cronica',\n",
    "                   'centros_rural', 'es_juntos']\n",
    "\n",
    "obs1 = [True, 40, 1, 50, 60, True]\n",
    "obs2 = [True, 55, 2, 30, 55, True]\n",
    "obs3 = [False, 25, 3, 40, 40, True]\n",
    "\n",
    "obs4 = [True, 30, 2, 25, 55, False]\n",
    "\n",
    "data = [obs1, obs2, obs3, obs4]\n",
    "\n",
    "#ejemplo_df = pd.DataFrame(data = data, columns = nombres_columnas)\n",
    "\n",
    "ejemplo_df  = pd.DataFrame(data = data, columns = nombres_columnas)\n",
    "\n",
    "\n",
    "ejemplo_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por columnas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ejemplo cuna más de anterior clase\n",
    "df = pd.DataFrame()\n",
    "\n",
    "df['rural'] = [True, True, False, True, False]\n",
    "df['pobreza'] = [40, 55, 25, 60, 30]\n",
    "df['rnum_ccpp_urbano'] =[1,2,3,4,5]\n",
    "df['rdesnutricion_cronica'] = [50,30,40,20,45]\n",
    "df['rcentros_rural'] = [60,55,40,50,20]\n",
    "df['res_juntos'] = [True, True, True, True, True]\n",
    "\n",
    "\n",
    "## df['cuna_o_no'] = [False, True, True, False, True]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización:\n",
    "La vectorización se define como aplicar una operación sobre el conjunto de elementos de un vector, en lugar de hacerlo sobre cada elemento de forma individual. \n",
    "Pandas integra dicha vectorización tal que las operaciones se realizan más rápida, fácil y eficientemente. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de vectorización:\n",
    "## Nótese: Aquí creamos un dataframe con un diccionario, donde las llaves son los nombres de las columnas y\n",
    "#  los valores son listas con los valores de cada columna.\n",
    "df = pd.DataFrame({'Vec': [1, 2, 3, 4, 5],\n",
    "                   'NonVec': [1, 2, 3, 4, 5]})\n",
    "\n",
    "# No vectorizado: Se utiliza un bucle para realizar la operación. \n",
    "for i in range(len(df)):\n",
    "    df.at[i, 'NonVec'] += 10\n",
    "\n",
    "print(df)\n",
    "# Vewctorizado: Se llama al nombre de la variable como si fuera un vector.\n",
    "df['Vec'] = df['Vec'] + 10\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Leyendo la información:  \n",
    "Los archivos que tenemos pueden estar en distintos formatos: dta, sav, json, csv, txt. Pandas puede leer cualquiera de dichos tipos de archivos. \n",
    "Lo importante es que en esencia, todos ellos representan datos **tabulares**, o datos que tienen \n",
    "\n",
    "\n",
    "| Método de pandas según file | Tipo de archivo        | Descripción                              |\n",
    "|------------------|------------------------|------------------------------------------|\n",
    "| `read_csv()`     | CSV                    | Lee archivos de valores separados por comas (CSV). |\n",
    "| `read_json()`    | JSON                   | Lee datos estructurados como JSON. |\n",
    "| `read_stata()`   | Stata                  | Lee archivos de datos en Stata.          |\n",
    "| `read_spss()`    | SPSS                   | Lee archivos de datos en SPSS.           |\n",
    "| `read_excel()`   | Excel (xls, xlsx)      | Lee archivos Excel.              |\n",
    "| `read_pickle()`  | Pickle                 | Lee objetos Python serializados almacenados en archivos Pickle. |\n",
    "| `read_parquet()` | Parquet                | Lee archivos Parquet.         |\n",
    "| `read_feather()` | Feather                | Lee archivos Feather.         |\n",
    "| `read_sql()`     | SQL Query / Database   | Lee de una base de datos usando una consulta SQL. |\n",
    "| `read_sas()`     | SAS                    | Lee archivos de datos de SAS.            |\n",
    "| `read_hdf()`     | HDF5                   | Lee archivos HDF5.            |\n",
    "| `read_orc()`     | ORC                    | Lee archivos ORC.             |\n",
    "| `read_fwf()`     | Fixed-Width Formatted  | Lee archivos de texto con ancho fijo.    |\n",
    "\n",
    "- Los métodos están organizados según importancia. \n",
    "- Hay otros métodos como `read_html`, `read_orc`, y `read_fwf`. Otros paquetes adicionales leen, por ejemplo, archivos .RData (R). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafíos comunes con archivos de datos. \n",
    "\n",
    "Recibir datos de terceros va a ser muy habitual en la labor de data scientists. Algunas situaciones a tener en cuenta (para el futuro), en particular con exceles y csvs, son las siguientes:\n",
    "\n",
    "Con CSV:\n",
    "- Problemas con los delimitadores: Lo común es  que el delimitador sea la coma `,`, sin embargo,`|`,  `;` ó incluso `Tab` pueden aparecer, lo que  originará errores de lectura si no se especifica el delimitador correcto. \n",
    "- Comillas que no cierran: `\"\"\"  \"\"`. \n",
    "- Codificación diferente: Datos en español en una codificación anglosajona (e.g.) ocasionará caracteres extraños. \n",
    "- Datos faltantes: la falta de comas puede ocasionar errores en cargar los datos. \n",
    "- Saltos de línea no sistemáticos\n",
    "\n",
    "Con Excel:\n",
    "- Datos no tabulares: Exceles que no tienen datos tabulares pueden ocasionar errores de lectura. \n",
    "- Más de un valor por celda. \n",
    "- Fórmulas de excel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trabajando con datos\n",
    "\n",
    "Hoy nos enfocaremos en la base de datos de postulantes de la Universidad Nacional de Ingeniería: \n",
    "https://www.datosabiertos.gob.pe/dataset/postulantes-al-concurso-de-admisi%C3%B3n-de-la-universidad-nacional-de-ingenier%C3%ADa-uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración inicial \n",
    "\n",
    "Los siguientes son métodos/atributos de exploración que aplicamos sobre el dataframe mismo. \n",
    "\n",
    "| Método / Atributo         | Descripción                                                                                             |\n",
    "|-----------------|---------------------------------------------------------------------------------------------------------|\n",
    "| `head(n)`/`tail(n)` | Muestra las primeras/últimas n filas del DataFrame.\n",
    "| `describe()`    | Da estadísticas como el promedio, desviación estándar, mínimo y máximo de **columnas numéricas**. |\n",
    "| `info()`        | Nos ofrece información importante como los tipos de los atributos y sus valores nulos.     |\n",
    "| `dtypes`        | Da los tipos de datos del dataframe                                             |\n",
    "| `shape`         | Informa sobre la dimensión del dataframe en forma de tupla `(nfilas, ncolumnas)` |\n",
    "| `columns`         | Informa sobre el nombre de las columnas del dataframe. |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Leyendo los archivos. \n",
    "locacion_datos = \"aqui  coloca el string con la locación de tus datos\"\n",
    "locacion_datos = \"/Users/ccsuehara/Downloads/Datos_abiertos_admision_2021_1_2024_1.csv\" # este es el mio\n",
    "df = pd.read_csv(locacion_datos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### Explorando los datos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe()\n",
    "#df.info()\n",
    "#df.head()\n",
    "#df.tail(4)\n",
    "\n",
    "#df.dtypes\n",
    "#df.shape\n",
    "#df.columnns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora exploraremos una columna en particular. Aquí se utilizará la exploración para una columna determinada (luego veremos que estas son Series). \n",
    "\n",
    "| Método          | Descripción                                                                                       |\n",
    "|-----------------|---------------------------------------------------------------------------------------------------|\n",
    "| `unique()`      | Da los valores únicos de la serie.                                                          |\n",
    "| `nunique()`     | Cuenta cuántos elementos únicos hay en la serie.                                                 |\n",
    "| `sort_values()` | Ordena la serie.         |\n",
    "| `value_counts()`| Da distribución de frecuencias de cada valor único de la serie.                                             |\n",
    "\n",
    "\n",
    "Para datos numéricos:                                                    \n",
    "`sum()`, `mean()`,  `min()`, `max()`, `mode()`, `median()` ,  `std()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejercicios: \n",
    "- Averigua cuáles son los 5 primeros departamentos en donde nacen los postulantes. \n",
    "- Cuenta cuántos hombres/mujeres postulan a la UNI. \n",
    "- Cuenta cuántos ingresan.  \n",
    "Hint: value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cómo seleccionar datos\n",
    "\n",
    "A veces sólo necesitamos entender una parte de todos nuestros datos. Por ejemplo, queremos ver los datos de los partidos políticos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['ANIO_NACIMIENTO'].unique()\n",
    "# df['ANIO_NACIMIENTO'].nunique()\n",
    "# df['NACIMIENTO_DEPA'].value_counts()\n",
    "# df['NACIMIENTO_DEPA'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un dataframe está compuesto de varias series: una columna es una serie diferente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df['ANIO_NACIMIENTO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de columnas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# por nombre: \n",
    "peque_lst = ['ANIO_NACIMIENTO', 'NACIMIENTO_DEPA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df[peque_lst].copy()\n",
    "df_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección y filtración de filas/columnas con ``` iloc``` y ```loc ```\n",
    "\n",
    "Primero seleccionaremos columnas, para lo que utilizaremos  ``` iloc``` y ```loc ```. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilizando el iloc\n",
    "El iloc ubica las observaciones que corresponden al índice que le indicamos. \n",
    "Recordando nuestra base de congresistas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agarrando observaciones puntuales\n",
    "\n",
    "df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agarrando observaciones puntuales\n",
    "rows = [1, 2, 10,13,3000]\n",
    "df.iloc[rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [10,13, 2]\n",
    "cols = [2,4,6]\n",
    "df.iloc[rows, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qué pasa si  hago...\n",
    "#df.iloc[rows, ['COLEGIO_DEPA', 'COLEGIO_PROV', 'COLEGIO_ANIO_EGRESO']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:3,:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilizando el ```loc```\n",
    "\n",
    "El ``` loc``` es una forma de ubicar observaciones que se basa en las etiquetas, tanto de columnas como de filas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas = ['COLEGIO_DEPA', 'COLEGIO_PROV', 'COLEGIO_ANIO_EGRESO']\n",
    "filas = range(0,6)\n",
    "df.loc[filas, columnas]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, columnas]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrando información\n",
    "\n",
    "Muchas veces querremos quedarnos con un subconjunto de datos que cumplen cierta condición. Las condiciones tienen que evaluarse a un booleano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrando información con condiciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condicion = [True]*(len(df) - 5) + [False] * (5) # Tenemos una lista de booleanons de la longitud del dataframe. \n",
    "df[condicion] # Filtramos el dataframe con la condición. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = df['COLEGIO_DEPA'] == 'LIMA'\n",
    "# explora qué bota el cond. \n",
    "\n",
    "df[cond]\n",
    "# df[cond].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_1 = df['COLEGIO_DEPA'] == \"LIMA\"\n",
    "cond_2 = df['INGRESO'] == \"NO\"\n",
    "\n",
    "cond_filtrado_and = df[cond_1 & cond_2] ## Pregunta: por qué no usamos el and y or, como habíamos visto antes? \n",
    "\n",
    "cond_filtrado_or = df[cond_1 | cond_2]\n",
    "\n",
    "## equivalente a: \n",
    "df[(df['COLEGIO_DEPA'] == \"LIMA\") & (df['INGRESO'] == \"NO\")] ## necesitamos cerrarlo en paréntesis. \n",
    "\n",
    "df[(df['COLEGIO_DEPA'] == \"LIMA\") | (df['INGRESO'] == \"NO\")]\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-2]",
   "language": "python",
   "name": "conda-env-anaconda3-2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
